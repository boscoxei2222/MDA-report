{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook focuses on preprocessing the data for our AED placement optimization project.\n",
    "\n",
    "## 1. Import Libraries and Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path: /Users/Zhuanz/数字游民/mda 项目/mda_project\n",
      "Raw data path: /Users/Zhuanz/数字游民/mda 项目/mda_project/data/raw\n",
      "Processed data path: /Users/Zhuanz/数字游民/mda 项目/mda_project/data/processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "import os\n",
    "\n",
    "# Set base path\n",
    "base_path = \"/Users/Zhuanz/数字游民/mda 项目/mda_project\"\n",
    "\n",
    "# Set up data paths\n",
    "raw_data_path = os.path.join(base_path, \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(base_path, \"data\", \"processed\")\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "print(\"Base path:\", base_path)\n",
    "print(\"Raw data path:\", raw_data_path)\n",
    "print(\"Processed data path:\", processed_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We'll load the raw data files and handle any potential missing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AED total coordinates loaded successfully.\n",
      "MUG1 data loaded successfully.\n",
      "Belgium shapefile loaded successfully.\n",
      "aed: 15227 rows, 11 columns\n",
      "ambulance: 279 rows, 9 columns\n",
      "mug: 94 rows, 10 columns\n",
      "pit: 24 rows, 8 columns\n",
      "interventions1: 200627 rows, 46 columns\n",
      "interventions2: 200627 rows, 46 columns\n",
      "interventions3: 200627 rows, 46 columns\n",
      "interventions4: 115647 rows, 45 columns\n",
      "interventions5: 38620 rows, 36 columns\n",
      "cad: 289401 rows, 35 columns\n",
      "aed_total: 15226 rows, 13 columns\n",
      "mug1: 73 rows, 12 columns\n",
      "belgium_with_provinces_boundary: 11 rows, 9 columns\n"
     ]
    }
   ],
   "source": [
    "def safe_read_parquet(file_path):\n",
    "    try:\n",
    "        return pd.read_parquet(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load raw data\n",
    "aed = safe_read_parquet(os.path.join(raw_data_path, \"aed_locations.parquet.gzip\"))\n",
    "ambulance = safe_read_parquet(os.path.join(raw_data_path, \"ambulance_locations.parquet.gzip\"))\n",
    "mug = safe_read_parquet(os.path.join(raw_data_path, \"mug_locations.parquet.gzip\"))\n",
    "pit = safe_read_parquet(os.path.join(raw_data_path, \"pit_locations.parquet.gzip\"))\n",
    "interventions1 = safe_read_parquet(os.path.join(raw_data_path, \"interventions1.parquet.gzip\"))\n",
    "interventions2 = safe_read_parquet(os.path.join(raw_data_path, \"interventions2.parquet.gzip\"))\n",
    "interventions3 = safe_read_parquet(os.path.join(raw_data_path, \"interventions3.parquet.gzip\"))\n",
    "interventions4 = safe_read_parquet(os.path.join(raw_data_path, \"interventions_bxl.parquet.gzip\"))\n",
    "interventions5 = safe_read_parquet(os.path.join(raw_data_path, \"interventions_bxl2.parquet.gzip\"))\n",
    "cad = safe_read_parquet(os.path.join(raw_data_path, \"cad9.parquet.gzip\"))\n",
    "\n",
    "# Load additional data (if available)\n",
    "try:\n",
    "    aed_total = pd.read_csv(os.path.join(processed_data_path, \"aed_total_coordinates.csv\"))\n",
    "    print(\"AED total coordinates loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: aed_total_coordinates.csv not found. Skipping this file.\")\n",
    "    aed_total = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    mug1 = pd.read_csv(os.path.join(processed_data_path, \"mug1.csv\"))\n",
    "    print(\"MUG1 data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: mug1.csv not found. Skipping this file.\")\n",
    "    mug1 = pd.DataFrame()\n",
    "\n",
    "# Load Belgium shapefile\n",
    "belgium_shapefile_path = os.path.join(raw_data_path, \"BELGIUM_-_Provinces.geojson\")\n",
    "try:\n",
    "    belgium_with_provinces_boundary = gpd.read_file(belgium_shapefile_path)\n",
    "    belgium_with_provinces_boundary = belgium_with_provinces_boundary.to_crs(4326)\n",
    "    print(\"Belgium shapefile loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load Belgium shapefile. Error: {e}\")\n",
    "    print(\"Proceeding without Belgium shapefile. Some geographic analyses may be limited.\")\n",
    "    belgium_with_provinces_boundary = gpd.GeoDataFrame()\n",
    "\n",
    "# Print summary of loaded data\n",
    "data_frames = {\n",
    "    'aed': aed,\n",
    "    'ambulance': ambulance,\n",
    "    'mug': mug,\n",
    "    'pit': pit,\n",
    "    'interventions1': interventions1,\n",
    "    'interventions2': interventions2,\n",
    "    'interventions3': interventions3,\n",
    "    'interventions4': interventions4,\n",
    "    'interventions5': interventions5,\n",
    "    'cad': cad,\n",
    "    'aed_total': aed_total,\n",
    "    'mug1': mug1,\n",
    "    'belgium_with_provinces_boundary': belgium_with_provinces_boundary\n",
    "}\n",
    "\n",
    "for name, df in data_frames.items():\n",
    "    if isinstance(df, (pd.DataFrame, gpd.GeoDataFrame)):\n",
    "        if not df.empty:\n",
    "            print(f\"{name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        else:\n",
    "            print(f\"{name}: Empty DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Preprocessing Functions\n",
    "\n",
    "Here we define the functions needed for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def correct_latitude(lat):\n",
    "    if pd.isna(lat):\n",
    "        return lat\n",
    "    if isinstance(lat, (str, float, int)):\n",
    "        lat_str = str(lat)\n",
    "        lat_str = re.sub(r'[^0-9-]', '', lat_str)\n",
    "        if len(lat_str) > 2:\n",
    "            lat_str = lat_str[:2] + '.' + lat_str[2:]\n",
    "        return float(lat_str)\n",
    "    return lat\n",
    "\n",
    "def correct_longitude(lon):\n",
    "    if pd.isna(lon):\n",
    "        return lon\n",
    "    if isinstance(lon, (str, float, int)):\n",
    "        lon_str = str(lon)\n",
    "        lon_str = re.sub(r'[^0-9-]', '', lon_str)\n",
    "        if len(lon_str) > 1:\n",
    "            lon_str = lon_str[:1] + '.' + lon_str[1:]\n",
    "        return float(lon_str)\n",
    "    return lon\n",
    "\n",
    "def is_within_belgium(lat, lon):\n",
    "    belgium_boundaries = {\n",
    "        'min_latitude': 49.50,\n",
    "        'max_latitude': 51.50,\n",
    "        'min_longitude': 2.5,\n",
    "        'max_longitude': 6.5\n",
    "    }\n",
    "    return (belgium_boundaries['min_latitude'] <= lat <= belgium_boundaries['max_latitude']) and \\\n",
    "           (belgium_boundaries['min_longitude'] <= lon <= belgium_boundaries['max_longitude'])\n",
    "\n",
    "def assign_province(df):\n",
    "    if 'geometry' not in belgium_with_provinces_boundary.columns:\n",
    "        print(\"Warning: Belgium shapefile is not properly loaded. Skipping province assignment.\")\n",
    "        df['Province'] = np.nan\n",
    "        return df\n",
    "    \n",
    "    geometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    joined_gdf = gpd.sjoin(gdf, belgium_with_provinces_boundary, how=\"left\", predicate=\"intersects\")\n",
    "    df['Province'] = joined_gdf['NAME_2']\n",
    "    return df\n",
    "\n",
    "def assign_nearest_province(df, belgium_with_provinces_boundary):\n",
    "    if 'geometry' not in belgium_with_provinces_boundary.columns:\n",
    "        print(\"Warning: Belgium shapefile is not properly loaded. Skipping nearest province assignment.\")\n",
    "        return df\n",
    "    \n",
    "    geometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    joined_gdf = gpd.sjoin(gdf, belgium_with_provinces_boundary, how=\"left\", predicate=\"intersects\")\n",
    "    df['Province'] = joined_gdf['NAME_2']\n",
    "    unassigned_gdf = gdf[df['Province'].isnull()].copy()\n",
    "    \n",
    "    if not unassigned_gdf.empty:\n",
    "        projected_gdf = unassigned_gdf.to_crs(epsg=31370)\n",
    "        projected_provinces = belgium_with_provinces_boundary.to_crs(epsg=31370)\n",
    "        nearest_provinces = []\n",
    "        \n",
    "        for idx, point in projected_gdf.iterrows():\n",
    "            distances = projected_provinces.geometry.distance(point.geometry)\n",
    "            nearest_idx = distances.idxmin()\n",
    "            nearest_province = projected_provinces.loc[nearest_idx, 'NAME_2']\n",
    "            nearest_provinces.append(nearest_province)\n",
    "        \n",
    "        unassigned_gdf['Province'] = nearest_provinces\n",
    "        df.loc[unassigned_gdf.index, 'Province'] = unassigned_gdf['Province']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Intervention Data\n",
    "\n",
    "In this section, we'll preprocess the intervention datasets, handling potential missing columns and data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interventions1.parquet.gzip\n",
      "Original shape: (200627, 46)\n",
      "Processed shape: (200627, 46)\n",
      "Processing interventions2.parquet.gzip\n",
      "Original shape: (200627, 46)\n",
      "Processed shape: (200627, 46)\n",
      "Processing interventions3.parquet.gzip\n",
      "Original shape: (200627, 46)\n",
      "Processed shape: (200627, 46)\n",
      "Processing interventions_bxl.parquet.gzip\n",
      "Original shape: (115647, 45)\n",
      "Processed shape: (115647, 45)\n",
      "Processing interventions_bxl2.parquet.gzip\n",
      "Original shape: (38620, 36)\n",
      "Processed shape: (38620, 36)\n",
      "Processing cad9.parquet.gzip\n",
      "Original shape: (289401, 35)\n",
      "Processed shape: (289401, 35)\n",
      "\n",
      "Combined interventions shape: (1045549, 74)\n",
      "\n",
      "Column names:\n",
      "['mission_id', 'service_name', 'postalcode_permanence', 'cityname_permanence', 'streetname_permanence', 'housenumber_permanence', 'latitude_permanence', 'longitude_permanence', 'permanence_short_name', 'permanence_long_name', 'vector_type', 'eventtype_firstcall', 'eventlevel_firstcall', 'eventtype_trip', 'eventlevel_trip', 'postalcode_intervention', 'cityname_intervention', 'latitude_intervention', 'longitude_intervention', 'province_intervention', 't0', 't1', 't1confirmed', 't2', 't3', 't4', 't5', 't6', 't7', 't9', 'intervention_time_(t1reported)', 'intervention_time_(t1confirmed)', 'waiting_time', 'intervention_duration', 'departure_time_(t1reported)', 'departure_time_(t1confirmed)', 'unavailable_time', 'name_destination_hospital', 'postalcode_destination_hospital', 'cityname_destination_hospital', 'streetname_destination_hospital', 'housenumber_destination_hospital', 'calculated_traveltime_destinatio', 'calculated_distance_destination', 'number_of_transported_persons', 'abandon_reason', 'intervention_time_t1reported', 'departure_time_t1reported', 'calculated_traveltime_departure_', 'calculated_distance_departure_to', 'calculated_distance_destination_', 'description_nl', 'ic_description_nl', 'eventtype_and_eventlevel', 'creationtime', 'permanence_long_name_nl', 'permanence_long_name_fr', 'permanence_short_name_nl', 'permanence_short_name_fr', 'service_name_nl', 'service_name_fr', 'vector_type_nl', 'vector_type_fr', 'abandon_reason_nl', 'abandon_reason_fr', 'province', 'eventsubtype_trip', 'citysectionname_intervention', 'province_invervention', 'ui', 'id', 'mission_nr', 'ambucode', 'unit_id']\n",
      "\n",
      "Data types:\n",
      "mission_id                 int64\n",
      "service_name              object\n",
      "postalcode_permanence    float64\n",
      "cityname_permanence       object\n",
      "streetname_permanence     object\n",
      "                          ...   \n",
      "ui                        object\n",
      "id                       float64\n",
      "mission_nr               float64\n",
      "ambucode                 float64\n",
      "unit_id                   object\n",
      "Length: 74, dtype: object\n",
      "\n",
      "Processed interventions data saved to: /Users/Zhuanz/数字游民/mda 项目/mda_project/data/processed/processed_interventions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "base_path = \"/Users/Zhuanz/数字游民/mda 项目/mda_project\"\n",
    "raw_data_path = os.path.join(base_path, \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(base_path, \"data\", \"processed\")\n",
    "\n",
    "# Ensure the processed data path exists\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "def safe_read_parquet(file_path):\n",
    "    try:\n",
    "        return pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read file {file_path}. Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preprocess_interventions(df, name):\n",
    "    print(f\"Processing {name}\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    print(f\"Processed shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load and process data\n",
    "intervention_files = [\n",
    "    \"interventions1.parquet.gzip\",\n",
    "    \"interventions2.parquet.gzip\",\n",
    "    \"interventions3.parquet.gzip\",\n",
    "    \"interventions_bxl.parquet.gzip\",\n",
    "    \"interventions_bxl2.parquet.gzip\",\n",
    "    \"cad9.parquet.gzip\"\n",
    "]\n",
    "\n",
    "processed_interventions = []\n",
    "\n",
    "for file in intervention_files:\n",
    "    df = safe_read_parquet(os.path.join(raw_data_path, file))\n",
    "    if not df.empty:\n",
    "        processed_df = preprocess_interventions(df, file)\n",
    "        processed_interventions.append(processed_df)\n",
    "\n",
    "# Combine all processed data\n",
    "if processed_interventions:\n",
    "    combined_interventions = pd.concat(processed_interventions, ignore_index=True)\n",
    "    print(f\"\\nCombined interventions shape: {combined_interventions.shape}\")\n",
    "    \n",
    "    # Display some basic information\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(combined_interventions.columns.tolist())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(combined_interventions.dtypes)\n",
    "    \n",
    "    # Save processed data\n",
    "    output_path = os.path.join(processed_data_path, \"processed_interventions.csv\")\n",
    "    combined_interventions.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed interventions data saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No intervention data to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess AED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Latitude'] = aed_total['latitude'].apply(correct_latitude)\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Longitude'] = aed_total['longitude'].apply(correct_longitude)\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Intervention'] = 0\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['AED'] = 1\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Eventlevel'] = np.nan\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['EventType'] = 'AED'\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['T3-T0'] = pd.NaT\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Ambulance'] = aed_total['Mug'] = aed_total['PIT'] = 0\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Ambulance'] = aed_total['Mug'] = aed_total['PIT'] = 0\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Ambulance'] = aed_total['Mug'] = aed_total['PIT'] = 0\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Vector type'] = 'AED'\n",
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/933484483.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aed_total['Mission ID'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AED data preprocessed. Shape: (14006, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/z4xgk8sn7b99hz2zwwrwyj800000gp/T/ipykernel_25402/4065102938.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Province'] = joined_gdf['NAME_2']\n"
     ]
    }
   ],
   "source": [
    "if not aed_total.empty:\n",
    "    aed_total = aed_total[aed_total['public'].isin(['Y', 'y', 'Oui-Ja', 'Ja', 'Oui', 'J', np.nan])]\n",
    "    aed_total['Latitude'] = aed_total['latitude'].apply(correct_latitude)\n",
    "    aed_total['Longitude'] = aed_total['longitude'].apply(correct_longitude)\n",
    "    aed_total['Intervention'] = 0\n",
    "    aed_total['AED'] = 1\n",
    "    aed_total['Eventlevel'] = np.nan\n",
    "    aed_total['EventType'] = 'AED'\n",
    "    aed_total['T3-T0'] = pd.NaT\n",
    "    aed_total['Ambulance'] = aed_total['Mug'] = aed_total['PIT'] = 0\n",
    "    aed_total['Vector type'] = 'AED'\n",
    "    aed_total['Mission ID'] = 0\n",
    "    aed_total = assign_province(aed_total)\n",
    "    aed_total = aed_total[['Mission ID', 'Latitude', 'Longitude', 'Intervention', 'Eventlevel', 'T3-T0', 'EventType', \n",
    "                           'Vector type', 'AED', 'Ambulance', 'Mug', 'PIT', 'Province']]\n",
    "    print(\"AED data preprocessed. Shape:\", aed_total.shape)\n",
    "else:\n",
    "    print(\"AED data is empty. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocess Ambulance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambulance data preprocessed. Shape: (279, 13)\n"
     ]
    }
   ],
   "source": [
    "if not ambulance.empty:\n",
    "    ambulance['Latitude'] = ambulance['latitude'].apply(correct_latitude)\n",
    "    ambulance['Longitude'] = ambulance['longitude'].apply(correct_longitude)\n",
    "    ambulance['Intervention'] = 0\n",
    "    ambulance['AED'] = 0\n",
    "    ambulance['Eventlevel'] = np.nan\n",
    "    ambulance['EventType'] = 'Ambulance'\n",
    "    ambulance['T3-T0'] = pd.NaT\n",
    "    ambulance['Ambulance'] = 1\n",
    "    ambulance['Mug'] = ambulance['PIT'] = 0\n",
    "    ambulance['Vector type'] = 'Ambulance'\n",
    "    ambulance['Mission ID'] = 0\n",
    "    ambulance = assign_province(ambulance)\n",
    "    ambulance = ambulance[['Mission ID', 'Latitude', 'Longitude', 'Intervention', 'Eventlevel', 'T3-T0', 'EventType', \n",
    "                           'Vector type', 'AED', 'Ambulance', 'Mug', 'PIT', 'Province']]\n",
    "    print(\"Ambulance data preprocessed. Shape:\", ambulance.shape)\n",
    "else:\n",
    "    print(\"Ambulance data is empty. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocess MUG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUG data preprocessed. Shape: (73, 13)\n"
     ]
    }
   ],
   "source": [
    "if not mug1.empty:\n",
    "    mug1['Latitude'] = mug1['latitude'].apply(correct_latitude)\n",
    "    mug1['Longitude'] = mug1['longitude'].apply(correct_longitude)\n",
    "    mug1['Intervention'] = 0\n",
    "    mug1['AED'] = 0\n",
    "    mug1['Eventlevel'] = np.nan\n",
    "    mug1['EventType'] = 'Mug'\n",
    "    mug1['T3-T0'] = pd.NaT\n",
    "    mug1['Ambulance'] = mug1['PIT'] = 0\n",
    "    mug1['Mug'] = 1\n",
    "    mug1['Vector type'] = 'MUG'\n",
    "    mug1['Mission ID'] = 0\n",
    "    mug1 = assign_province(mug1)\n",
    "    mug1 = mug1[['Mission ID', 'Latitude', 'Longitude', 'Intervention', 'Eventlevel', 'T3-T0', 'EventType', \n",
    "                 'Vector type', 'AED', 'Ambulance', 'Mug', 'PIT', 'Province']]\n",
    "    print(\"MUG data preprocessed. Shape:\", mug1.shape)\n",
    "else:\n",
    "    print(\"MUG data is empty. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preprocess PIT Data\n",
    "\n",
    "This section focuses on preprocessing the PIT (Paramedic Intervention Team) data, which requires special handling due to its unique structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIT locations data loaded successfully.\n",
      "Sample of PIT locations data:\n",
      "                           Base                   Medical resource  \\\n",
      "0             BA ESSE AZ Klina         PIT ESSEN (Team: PAESSE01A)   \n",
      "1        BH CHAR ISPPC Mambourg  PIT CHARLEROI 1 (Team: PHCHAR04A)   \n",
      "2        FN PDS METT Val Sambre       PIT METTET (Team: PNMETT01A)   \n",
      "3  FX PDS BOUI Luxembourg | PIT   PIT.BOUILLON 1 (Team: PXBOUI01A)   \n",
      "4  FX PDS VIRT Luxembourg | PIT     PIT.VIRTON 1 (Team: PXVIRT01A)   \n",
      "\n",
      "   Nr. Fonction PIT                          Lieu de départ    Province  \\\n",
      "0            104101               Kerkeneind 1 - 2910 ESSEN      Anvers   \n",
      "1            613001  Boulevard Zoé Drion 1 - 6000 CHARLEROI     Hainaut   \n",
      "2            620001         Rue Hennevauch 91 - 5640 METTET       Namur   \n",
      "3            903001    Rue des 4 moineaux 23B - 6832 CURFOZ  Luxembourg   \n",
      "4            902001      Rue d'Harnoncourt 48 - 6762 VIRTON  Luxembourg   \n",
      "\n",
      "     Région   Latitude  Longitude  \n",
      "0   Flandre  51.471093   4.464423  \n",
      "1  Wallonie  50.412746   4.451464  \n",
      "2  Wallonie  50.307537   4.661530  \n",
      "3  Wallonie  49.811719   5.080515  \n",
      "4  Wallonie  49.553189   5.526090  \n",
      "PIT data or PIT locations data is empty. Skipping preprocessing.\n",
      "No preprocessed PIT data available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_pit_data(pit, pit_locations):\n",
    "    if pit.empty or pit_locations.empty:\n",
    "        print(\"PIT data or PIT locations data is empty. Skipping preprocessing.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"Columns in PIT data:\", pit.columns)\n",
    "    print(\"Columns in PIT locations data:\", pit_locations.columns)\n",
    "\n",
    "    # Extract ID from Medical resource in pit_locations dataframe\n",
    "    pit_locations['id'] = pit_locations['Medical resource'].str.extract(r'Team: (\\w+)')\n",
    "    \n",
    "    # Add required columns\n",
    "    pit_locations['Intervention'] = 0\n",
    "    pit_locations['AED'] = 0\n",
    "    pit_locations['Eventlevel'] = np.nan\n",
    "    pit_locations['EventType'] = 'PIT'\n",
    "    pit_locations['T3-T0'] = pd.NaT\n",
    "    pit_locations['Ambulance'] = 0\n",
    "    pit_locations['Mug'] = 0\n",
    "    pit_locations['PIT'] = 1\n",
    "    pit_locations['Vector type'] = 'PIT'\n",
    "    pit_locations['Mission ID'] = pit_locations['Nr. Fonction PIT']\n",
    "\n",
    "    # Rename columns\n",
    "    pit_locations = pit_locations.rename(columns={\n",
    "        'Latitude': 'Latitude',\n",
    "        'Longitude': 'Longitude',\n",
    "        'Province': 'Province'\n",
    "    })\n",
    "\n",
    "    # Select required columns\n",
    "    columns = ['Mission ID', 'Latitude', 'Longitude', 'Intervention', 'Eventlevel', 'T3-T0', \n",
    "               'EventType', 'Vector type', 'AED', 'Ambulance', 'Mug', 'PIT', 'Province']\n",
    "    \n",
    "    pit_preprocessed = pit_locations.reindex(columns=columns)\n",
    "\n",
    "    print(\"PIT data preprocessed. Shape:\", pit_preprocessed.shape)\n",
    "    return pit_preprocessed\n",
    "\n",
    "# Load PIT locations data\n",
    "pit_locations_path = os.path.join(base_path, \"data\", \"processed\", \"pit_01022024_fr.xlsx\")\n",
    "try:\n",
    "    pit_locations = pd.read_excel(pit_locations_path)\n",
    "    print(\"PIT locations data loaded successfully.\")\n",
    "    print(\"Sample of PIT locations data:\")\n",
    "    print(pit_locations.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: PIT locations file not found at {pit_locations_path}\")\n",
    "    pit_locations = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PIT locations data: {e}\")\n",
    "    pit_locations = pd.DataFrame()\n",
    "\n",
    "# Preprocess PIT data\n",
    "if not pit_locations.empty:\n",
    "    pit_preprocessed = preprocess_pit_data(pd.DataFrame(), pit_locations)  # We're not using the 'pit' dataframe\n",
    "\n",
    "    if not pit_preprocessed.empty:\n",
    "        print(\"Sample of preprocessed PIT data:\")\n",
    "        print(pit_preprocessed.head())\n",
    "        \n",
    "        # Save the preprocessed PIT data\n",
    "        output_path = os.path.join(processed_data_path, \"pit_preprocessed.csv\")\n",
    "        pit_preprocessed.to_csv(output_path, index=False)\n",
    "        print(f\"Preprocessed PIT data saved to: {output_path}\")\n",
    "    else:\n",
    "        print(\"No preprocessed PIT data available.\")\n",
    "else:\n",
    "    print(\"PIT locations data is not available or is empty. Skipping PIT data preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combine All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets combined. Shape of total_df: (21205, 13)\n",
      "\n",
      "Unique number of Latitude values: 15390\n",
      "Unique number of Longitude values: 15427\n",
      "Unique values eventlevels: [ 1.  2.  0.  5.  7.  6. nan]\n",
      "\n",
      "Cross table of Event Levels:\n",
      " col_0  count\n",
      "row_0       \n",
      "0.0     1540\n",
      "1.0     3989\n",
      "2.0      559\n",
      "5.0      713\n",
      "6.0       40\n",
      "7.0        6\n",
      "\n",
      "Maximum Response Time: nan\n",
      "\n",
      "Missing values per variable:\n",
      " Mission ID          0\n",
      "Latitude            0\n",
      "Longitude           0\n",
      "Intervention        0\n",
      "Eventlevel      14358\n",
      "T3-T0           21205\n",
      "EventType           0\n",
      "Vector type         0\n",
      "AED                 0\n",
      "Ambulance           0\n",
      "Mug                 0\n",
      "PIT                 0\n",
      "Province          101\n",
      "dtype: int64\n",
      "\n",
      "Total Length of the Dataset: 21205\n",
      "\n",
      "Types of variables in dataset:\n",
      " Mission ID        int64\n",
      "Latitude        float64\n",
      "Longitude       float64\n",
      "Intervention      int64\n",
      "Eventlevel      float64\n",
      "T3-T0            object\n",
      "EventType        object\n",
      "Vector type      object\n",
      "AED               int64\n",
      "Ambulance         int64\n",
      "Mug               int64\n",
      "PIT               int64\n",
      "Province         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Combine all preprocessed datasets\n",
    "all_datasets = [\n",
    "    interventions_TOTAL,\n",
    "    aed_total,\n",
    "    ambulance,\n",
    "    mug1,\n",
    "    pit_preprocessed\n",
    "]\n",
    "\n",
    "# Filter out any empty DataFrames\n",
    "all_datasets = [df for df in all_datasets if not df.empty]\n",
    "\n",
    "if all_datasets:\n",
    "    total_df = pd.concat(all_datasets, axis=0, ignore_index=True)\n",
    "    print(\"All datasets combined. Shape of total_df:\", total_df.shape)\n",
    "else:\n",
    "    print(\"Warning: No datasets available to combine. Creating an empty DataFrame.\")\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "# Print summary statistics\n",
    "if not total_df.empty:\n",
    "    print('\\nUnique number of Latitude values:', len(total_df['Latitude'].unique()))\n",
    "    print('Unique number of Longitude values:', len(total_df['Longitude'].unique()))\n",
    "    print('Unique values eventlevels:', total_df['Eventlevel'].unique())\n",
    "\n",
    "    cross_tab = pd.crosstab(index=pd.Categorical(total_df[\"Eventlevel\"]), columns='count')\n",
    "    print('\\nCross table of Event Levels:\\n', cross_tab)\n",
    "\n",
    "    max_responseTime = total_df['T3-T0'].max()\n",
    "    print(f'\\nMaximum Response Time: {max_responseTime}')\n",
    "    print('\\nMissing values per variable:\\n', total_df.isnull().sum())\n",
    "    print('\\nTotal Length of the Dataset:', len(total_df))\n",
    "    print('\\nTypes of variables in dataset:\\n', total_df.dtypes)\n",
    "else:\n",
    "    print(\"The combined dataset is empty. Skipping summary statistics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
